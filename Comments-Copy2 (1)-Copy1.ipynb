{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#One Vs Rest Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import skmultilearn\n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from sklearn.metrics import hamming_loss, accuracy_score \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize,TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import os\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#library for EDA\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Library for saving data\n",
    "import pickle\n",
    "\n",
    "#library for GridSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#library for error detection in regression models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#libraries for regularisation\n",
    "from sklearn.linear_model import Ridge,Lasso,RidgeCV,LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "\n",
    "#show all columns in dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(df.columns.values)\n",
    "categories = categories[2:]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of comments in each category\n",
    "\n",
    "counts = []\n",
    "for category in categories:\n",
    "    counts.append((category, df[category].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"malignant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"highly_malignant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rude\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"threat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"abuse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loathe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of comments having multiple labels.\n",
    "rowSums = df.iloc[:,2:].sum(axis=1)\n",
    "multiLabel_counts = rowSums.value_counts()\n",
    "multiLabel_counts = multiLabel_counts.iloc[1:]\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
    "\n",
    "plt.title(\"Comments having multiple labels \")\n",
    "plt.ylabel('Number of comments', fontsize=18)\n",
    "plt.xlabel('Number of labels', fontsize=18)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = multiLabel_counts.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping id as its unique\n",
    "df.drop(columns=[\"id\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud representation of most used words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge wordcloud=1.6.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "# malignant\n",
    "subset = df[df.malignant==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_malignant = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Toxic\",fontsize=40)\n",
    "plt.imshow(cloud_malignant)\n",
    "\n",
    "\n",
    "# cloud_highly_malignant\n",
    "subset = df[df.highly_malignant==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_highly_malignant = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.axis('off')\n",
    "plt.title(\"highly_malignant\",fontsize=40)\n",
    "plt.imshow(cloud_highly_malignant)\n",
    "\n",
    "\n",
    "# rude\n",
    "subset = df[df.rude==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_rude = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.axis('off')\n",
    "plt.title(\"rude\",fontsize=40)\n",
    "plt.imshow(cloud_rude)\n",
    "\n",
    "\n",
    "# threat\n",
    "subset = df[df.threat==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_threat = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.axis('off')\n",
    "plt.title(\"Threat\",fontsize=40)\n",
    "plt.imshow(cloud_threat)\n",
    "\n",
    "\n",
    "# abuse\n",
    "subset = df[df.abuse==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_abuse = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.axis('off')\n",
    "plt.title(\"abuse\",fontsize=40)\n",
    "plt.imshow(cloud_abuse)\n",
    "\n",
    "\n",
    "# loathe\n",
    "subset = df[df.loathe==1]\n",
    "text = subset.comment_text.values\n",
    "cloud_loathe = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "plt.title(\"loathe\",fontsize=40)\n",
    "plt.imshow(cloud_loathe)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python Recursive Limitation = \", sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python Recursive Limitation = \", sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into 3000 rows\n",
    "df_1 = df.iloc[:100000,:]\n",
    "df_2 = df.iloc[3000:,:]\n",
    "print(\"Shape of new dataframes - {} , {}\".format(df_1.shape, df_2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laptops GPU is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(comments):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    comment = str(comments)\n",
    "    comment = comment.lower()\n",
    "    comment = comment.replace(\"<br /><br />\", \"\")\n",
    "    tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopwords_removed = [i for i in tokens if i not in stop_words]\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stem_text = [ps.stem(i) for i in stopwords_removed]\n",
    "\n",
    "    cleaned_comments = \" \".join(stem_text)\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def cleanlowercase(sentence):\n",
    "    cleaned = cleaned.str.lower()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [preprocess(i) for i in df.comment_text]\n",
    "b=[cleanHtml(j) for j in a]\n",
    "c=[cleanPunc(j) for j in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments']=c\n",
    "df.drop(columns=[\"comment_text\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.30, shuffle=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comments']\n",
    "test_text = test['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments'] = df['comments'].astype(np.uint8,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train.drop(labels = ['comments'], axis=1)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['comments'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "preddt=dt.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,preddt))\n",
    "print(confusion_matrix(y_test,preddt))\n",
    "print(classification_report(y_test,preddt))\n",
    "print(\"Hamming loss = \",hamming_loss(y_test,predrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "predrf=rf.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,predrf))\n",
    "print(confusion_matrix(y_test,predrf))\n",
    "print(classification_report(y_test,predrf))\n",
    "\n",
    "print(\"Hamming loss = \",hamming_loss(y_test,predrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "from sklearn.linear_model import SDGClassifer\n",
    "\n",
    "sgd=SDGClassifier()\n",
    "sgd.fit(x_train,y_train)\n",
    "predrf=sgd.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,predrf))\n",
    "print(confusion_matrix(y_test,predrf))\n",
    "print(classification_report(y_test,predrf))\n",
    "print(\"Hamming loss = \",hamming_loss(y_test,predrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "from sklearn.neighbors import KNeighborsClassifer\n",
    "\n",
    "kn=KNeighborsClassifier()\n",
    "kn.fit(x_train,y_train)\n",
    "predrf=kn.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,predrf))\n",
    "print(confusion_matrix(y_test,predrf))\n",
    "print(classification_report(y_test,predrf))\n",
    "print(\"Hamming loss = \",hamming_loss(y_test,predrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR=LogisticRegression()\n",
    "LR.fit(x_train,y_train)\n",
    "predlr=LR.predict(x_test)\n",
    "print(accuracy_score(y_test,predlr))\n",
    "print(confusion_matrix(y_test,predlr))\n",
    "print(classification_report(y_test,predlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation score\n",
    "#Finding overfitting\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scr=cross_val_score(LR,x,y,cv=5)\n",
    "print(\"Cross validation score of Logistic Regression model :\", scr.mean())\n",
    "\n",
    "scr=cross_val_score(dt,x,y,cv=5)\n",
    "print(\"Cross validation score of Decision Tree model :\", scr.mean())\n",
    "\n",
    "scr=cross_val_score(rf,x,y,cv=5)\n",
    "print(\"Cross validation score of Random Forest model :\", scr.mean())\n",
    "\n",
    "scr=cross_val_score(svc,x,y,cv=5)\n",
    "print(\"Cross validation score of SVC model :\", scr.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM doesn't support multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "paramters={'penalty':['l1', 'l2','elasticnet','none'],\n",
    "           'dual':[True,False],\n",
    "           'tol':[0.75,1],\n",
    "           'C':[2,3,4,7],\n",
    "           'intercept_scaling':[1.2,2.1],\n",
    "           'class_weight':['dict','balanced'],\n",
    "           'random_state':[2,3],\n",
    "          'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "           }\n",
    "\n",
    "\n",
    "GCV=GridSearchCV(LogisticRegression(),paramters,cv=6)\n",
    "GCV.fit(x_train,y_train)\n",
    "\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1=LogisticRegression(random_state=2,C=2,dual=False,intercept_scaling=2.1,penalty='l2',tol=0.75,class_weight='balanced',solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1.fit(x_train,y_train)\n",
    "pred=mod1.predict(x_test)\n",
    "print(\"LogisticRegression\")\n",
    "print(\"accuracy score\",accuracy_score(y_test,pred)*100)\n",
    "print(\"log loss score:\",log_loss(y_test,pred)*100)\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree CLassifier\n",
    "paramters={'criterion':['gini', 'entropy'],\n",
    "           'splitter':['best', 'random'],\n",
    "           'min_samples_split':[2,3,4,5],\n",
    "           'min_samples_leaf':[2,3,4,5,6],\n",
    "           'max_leaf_nodes':[2,3,4,5,10],\n",
    "}\n",
    "\n",
    "GCV=GridSearchCV(DecisionTreeClassifier(),paramters,cv=5)\n",
    "GCV.fit(x_train,y_train)\n",
    "\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2=DecisionTreeClassifier(criterion='gini',max_leaf_nodes=10,min_samples_leaf=4,min_samples_split=4,splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2.fit(x_train,y_train)\n",
    "pred=mod2.predict(x_test)\n",
    "print(\"DecisionTreeClassifier\")\n",
    "print(\"accuracy score\",accuracy_score(y_test,pred)*100)\n",
    "print(\"log loss score:\",log_loss(y_test,pred)*100)\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter tuning\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "paramters={'n_estimators':[2,3,4,5],\n",
    "           'criterion':['gini','entropy'],\n",
    "           'min_samples_split':[2,3,4,5],\n",
    "           'min_samples_leaf':[2,3,4,5,6],\n",
    "           'max_leaf_nodes':[2,3,4,5,10],\n",
    "}\n",
    "\n",
    "GCV=GridSearchCV(RandomForestClassifier(),paramters,cv=5)\n",
    "\n",
    "GCV.fit(x_train,y_train)\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3=RandomForestClassifier(criterion='gini',max_leaf_nodes=10,min_samples_leaf=3,min_samples_split=2,n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.fit(x_train,y_train)\n",
    "pred=mod3.predict(x_test)\n",
    "print(\"RandomForestClassifier\")\n",
    "print(\"accuracy score\",accuracy_score(y_test,pred)*100)\n",
    "print(\"log loss score:\",log_loss(y_test,pred)*100)\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on SDGClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter tuning\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "paramters={'penalty':['l2', 'l1', 'elasticnet'],\n",
    "           'criterion':['gini','entropy'],\n",
    "           'alpha':[2,3,4,5],\n",
    "           'l1_ratio':[2,3,4,5,6],\n",
    "           'max_iter':[2,3,4,5,10],\n",
    "           'fit_intercept':[True,False]\n",
    "}\n",
    "\n",
    "GCV=GridSearchCV(RandomForestClassifier(),paramters,cv=5)\n",
    "\n",
    "GCV.fit(x_train,y_train)\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4=RandomForestClassifier(criterion='gini',penalty=10,alpha=3,l1_ratio=2,max_iter=5,fit_intercept=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4.fit(x_train,y_train)\n",
    "pred=mod4.predict(x_test)\n",
    "print(\"RandomForestClassifier\")\n",
    "print(\"accuracy score\",accuracy_score(y_test,pred)*100)\n",
    "print(\"log loss score:\",log_loss(y_test,pred)*100)\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on KNeighborsClassifer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter tuning\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "paramters={'n_neighbors':[2,3,4,5],\n",
    "           'weights':['uniform', 'distance'],\n",
    "           'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "           'leaf_size':[2,3,4,5,6],\n",
    "           'p':[2,3,4,5,10],\n",
    "}\n",
    "\n",
    "GCV=GridSearchCV(RandomForestClassifier(),paramters,cv=5)\n",
    "\n",
    "GCV.fit(x_train,y_train)\n",
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5=RandomForestClassifier(n_neighbors='gini',weights=10,algorithm=3,leaf_size=2,p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5.fit(x_train,y_train)\n",
    "pred=mod5.predict(x_test)\n",
    "print(\"RandomForestClassifier\")\n",
    "print(\"accuracy score\",accuracy_score(y_test,pred)*100)\n",
    "print(\"log loss score:\",log_loss(y_test,pred)*100)\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
